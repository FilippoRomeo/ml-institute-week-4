# ğŸ–¼ï¸ Image Captioning Transformer

A Transformer-based image captioning system implemented from scratch in PyTorch. Supports both **Vision Transformer (ViT)** and **CLIP** as encoders and allows you to choose between a custom **SentencePiece tokenizer** or a pretrained **Hugging Face tokenizer** (`EleutherAI/pythia-160m`).

---

## âœ… Requirements

Tested on **Ubuntu 22.04** with:

- Python 3.11+
- CUDA-enabled GPU (optional but recommended)
- `conda` (for environment management)

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/FilippoRomeo/ml-institute-week-4.git
cd image_captioning_transformer
```

### 2. Create and Activate Conda Environment

```bash
conda create -n ai-lab python=3.11 -y
conda activate ai-lab
```

### 3. Install Required Packages

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets sentencepiece tqdm Pillow scikit-learn matplotlib
```

---

## ğŸ“š Dataset Setup

The Flickr30k dataset is automatically downloaded from Hugging Face:

```python
from datasets import load_dataset
dataset = load_dataset("nlphuji/flickr30k", split="test")
```

No manual download required.

---

## ğŸ”¤ Tokenizer Setup

### Option A: Use SentencePiece (default)

Generate captions.txt and train tokenizer:

```bash
python -c "from data.tokenizer import save_all_captions_to_txt, train_sentencepiece; save_all_captions_to_txt(); train_sentencepiece()"
```

This saves `spm.model` and `spm.vocab` in `data/tokenizer/`.

### Option B: Use Hugging Face Tokenizer

Switch to `HFTokenizerWrapper` in `data/tokenizer.py` and set model to `EleutherAI/pythia-160m`.

---

## ğŸ‹ï¸ Train the Model

To start training with ViT and SentencePiece:

```bash
python -m training.train
```

This will:

- Split Flickr30k into train/val/test (80/10/10)
- Train for 20 epochs
- Save model checkpoints to `checkpoints/`

To delete old checkpoints before retraining:

```bash
rm checkpoints/caption_model_epoch*.pt
```

---

## ğŸ–¼ï¸ Generate Captions from Image

After training, run:

```bash
python -m inference.generate --image inference/sample.jpg --checkpoint checkpoints/caption_model_epoch10.pt
```

Expected output:

```
ğŸ–¼ï¸ Caption: A man in a red shirt is riding a bicycle.
```

---

## ğŸ§  Architecture Overview

- **Encoder**:
  - ViT patch-based image encoder (default)
  - Optional: use CLIP for pretrained image features
- **Decoder**:
  - Transformer-based decoder implemented from scratch
- **Tokenizers**:
  - SentencePiece (trained on Flickr30k)
  - Hugging Face model tokenizer

---

## ğŸ—‚ï¸ Project Structure

```
image_captioning_transformer/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset.py
â”‚   â””â”€â”€ tokenizer.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ caption_model.py
â”‚   â”œâ”€â”€ decoder_transformer.py
â”‚   â””â”€â”€ encoder_vit.py
â”œâ”€â”€ training/
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ generate.py
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ (saved model checkpoints)
â””â”€â”€ README.md
```

---

## ğŸ§¹ Reset Local Code to Remote (if needed)

To reset your repo to match the latest version on GitHub:

```bash
git fetch origin
git reset --hard origin/main
```

---

## ğŸ“ License

MIT License
